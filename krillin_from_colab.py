# -*- coding: utf-8 -*-
"""krillin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oLJyJVUuIpEtTbWuNOdBx8PL1GFS3UO1
"""

!pip install -U -q PyDrive
!pip install torch==0.4.0
!pip install spacy
!python -m spacy download en_core_web_md

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

import torch
import torch.nn as nn
import spacy
import numpy as np
import json
from google.colab import files

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#1WYT_IUd0eZLiL45qGIIS78iLwh-laVgx
file_id = '1WYT_IUd0eZLiL45qGIIS78iLwh-laVgx'
downloaded = drive.CreateFile({'id': file_id})
downloaded.GetContentFile('test.json')

import numpy as np
import torch
import spacy
from torch.utils.data import Dataset, DataLoader
import json


class myDataset(Dataset):
	def __init__(self,filename):
		self.nlp=spacy.load("en_core_web_md")
		self.filename=filename
		self.length=0
		with open(filename) as fp:
			s=fp.read()
		self.arr=s.split('\n')
		self.y=1


	def __len__(self):
		if self.length == 0:
			for i in range(len(self.arr)-1):
				try:
					obj=json.loads(self.arr[i])
					self.length+=len(obj["sentences"])
				except ValueError:
					print('got a Value error(most probably improper JSON at '+str(i))
					pass
		return self.length
	
	def __getitem__(self,idx):
		index=0
		for i in range(len(self.arr)):
			obj=json.loads(self.arr[i])
			no_of_ans=len(obj["sentences"])
			
			if((index+no_of_ans)>idx) :
				break
			index+=no_of_ans
		if self.y :
			if (idx-index) in obj["candidates"]:
				label=1
			else:
				label=0
		question=self.nlp(obj["question"])
		answer=self.nlp(obj["sentences"][idx-index])
		list_of_tokens=[]
		for i in range(40):
			if(i<len(question)):
				list_of_tokens.append(torch.Tensor(question[i].vector).cuda(0))
			else:
				list_of_tokens.append(torch.zeros(300).cuda(0))
		for i in range(40):
			if(i<len(answer)):
    				list_of_tokens.append(torch.Tensor(answer[i].vector).cuda(0))
			else:
				list_of_tokens.append(torch.zeros(300).cuda(0))
		sentence_to_embedding=torch.stack(list_of_tokens,dim=0)
		return sentence_to_embedding,torch.tensor(label)

import numpy as np
import torch.nn as nn
import torch
from torch.autograd import Variable
from torch import optim
import torch.nn.functional as F
dtype = torch.float
device = torch.device("cuda:0")

class selqa_net(nn.Module):
	def __init__(self):

		super(selqa_net, self).__init__()

		self.conv1=torch.nn.Conv2d(1,10,5,stride=1,padding=1)
		self.pool1=torch.nn.MaxPool3d(2,stride=2,padding=0)
		self.pool2=torch.nn.MaxPool3d(3,stride=2,padding=0)
		self.dropout1=torch.nn.Dropout(p=0.1, inplace=False)
		self.linear=torch.nn.Linear(2812,1)

	def forward(self,x):
		x.type(torch.FloatTensor)
		x.unsqueeze_(1)
		x=F.relu(self.conv1(x))
		# print(x.shape)
		x=self.pool1(x)
		# print(x.shape)
		x=self.dropout1(x)
		# print(x.shape)
		x=self.pool2(x)
		# print(x.shape)
		x=x.view(100,-1)
		# print(x.shape)
		x=self.linear(x)
		x=x*10
		return x

import torch.nn as nn
import torch.utils as utils
import torch.optim as optim
import torch

# if __name__ == 'main':
    
    
# test_data=myDataset()
learning_rate=1e-4
train_dataset=myDataset('test.json')
trainloader=utils.data.DataLoader(train_dataset,batch_size=100,shuffle=True,num_workers=0)
criterion=nn.MSELoss(size_average=True)
model=selqa_net()
model=model.cuda()
optimizer=optim.Adam(model.parameters(),lr=learning_rate)


for epoch in range(50):  # loop over the dataset multiple times

    running_loss = 0.0
    # for i,j  in enumerate(trainloader):
    #     print(i,j)
    # print(trainloader)
    #print(enumerate(trainloader))
    for i, data in enumerate(trainloader, 0):
        # get the inputs
        inputs, labels = data
        inputs=inputs.cuda()
        labels=labels.cuda()
        labels=labels.type(torch.cuda.FloatTensor)
        labels.unsqueeze_(1)
        labels=labels*10
        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model(inputs).cuda()
        loss = criterion(outputs, labels).cuda()
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        # print(str(i)+" "+str(loss.item()))
        if i % 5 == 1:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.5f' %
                (epoch + 1, i + 1, running_loss / 5))
            running_loss = 0.0

print('Finished Training')

#saving my model
model.save_state_dict('mytraining.pt')
uploaded = drive.CreateFile({'title': 'krillin00.pt'})
uploaded.SetContentFile('mytraining.pt')
uploaded.Upload()
print('Uploaded file with ID {}'.format(uploaded.get('id')))