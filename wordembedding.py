# -*- coding: utf-8 -*-
"""wordEmbedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16Dkr60gS_9U2YmsvPk-bRQj4Reutkcyo
"""

!pip install torch==0.4.0

!pip install numpy
!pip install spacy

!python -m spacy download en_core_web_md

"""# Loading Data from Google Drive

install dependencies
"""

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

"""authenticate user"""

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

"""import file"""

import torch
x=torch.randn(3,5)
uploaded = drive.CreateFile({'title': 'randnTensor.pt'})
torch.save(x,'r.pt')


uploaded.SetContentFile('r.pt')

uploaded.Upload()
print('Uploaded file with ID {}'.format(uploaded.get('id')))

import torch
import torch.nn as nn
import spacy
import numpy as np
import json

max_no_of_tokens=40
no_missed=0
total_tokens=0
total_sents=0
q_missed=0
word_to_ix={}

#read data into dataset

nlp=spacy.load("en_core_web_md")
list_of_sents=[]

with open('SelQA-ass-train.json') as fp:
    s=fp.read()
arr=s.split('\n')
print(arr[0])
for i in range( len(arr)):
#     i=len(arr)
    try:
      list_of_tokens=[]
      obj=json.loads(arr[i])

      total_sents+=len(obj["sentences"])+1
      print(i)
      q=nlp(obj["question"])
      for i in range(40):
          if(i<len(q)):
              temp=torch.Tensor(q[i].vector).cuda(0)
              # print(temp.shape)
              list_of_tokens.append(temp)
          else:
              list_of_tokens.append(torch.zeros(300).cuda(0))
      tokens_question=list_of_tokens.copy()
      print(len(obj["sentences"]))
      for sentence in obj["sentences"]:

          list_of_tokens=tokens_question.copy()
          y=nlp(sentence)
          for i in range(40):
              if(i<len(y)):
                  temp=torch.Tensor(y[i].vector).cuda(0)
                  list_of_tokens.append(temp)
              else:
                  list_of_tokens.append(torch.zeros(300).cuda(0))

          # print(len(list_of_tokens))
          sentence_to_embedding=torch.stack(list_of_tokens,dim=0).cuda(0)
          list_of_sents.append(sentence_to_embedding)
    except:
          print("Decode error on line", i)
x=torch.stack(list_of_sents,0)
print(x.shape)

print(len(list_of_sents))

print(len(list_of_sents))
# for i in range(len(list_of_sents)//10):
x = torch.stack(list_of_sents[0:22000], 0)
torch.save(x,'first22K.pt')
files.download('first22K.pt')

torch.save(x,'first22K.pt')

files.download('first22K.pt')

max_no_of_tokens=40
no_missed=0
total_tokens=0
total_sents=0
q_missed=0
word_to_ix={}

#read data into dataset

nlp=spacy.load("en_core_web_md")
list_of_sents=[]

with open('SelQA-ass-train.json') as fp:
    s=fp.read()
arr=s.split('\n')
print(arr[0])
for i in range( len(arr)):
#     i=len(arr)
    try:
      list_of_tokens=[]
      obj=json.loads(arr[i])

      total_sents+=len(obj["sentences"])+1
      print(i)
      q=nlp(obj["question"])
      for i in range(40):
          if(i<len(q)):
              temp=torch.Tensor(q[i].vector).cuda(0)
              # print(temp.shape)
              list_of_tokens.append(temp)
          else:
              list_of_tokens.append(torch.zeros(300).cuda(0))
      tokens_question=list_of_tokens.copy()
      print(len(obj["sentences"]))
      for sentence in obj["sentences"]:

          list_of_tokens=tokens_question.copy()
          y=nlp(sentence)
          for i in range(40):
              if(i<len(y)):
                  temp=torch.Tensor(y[i].vector).cuda(0)
                  list_of_tokens.append(temp)
              else:
                  list_of_tokens.append(torch.zeros(300).cuda(0))

          # print(len(list_of_tokens))
          sentence_to_embedding=torch.stack(list_of_tokens,dim=0).cuda(0)
          list_of_sents.append(sentence_to_embedding)